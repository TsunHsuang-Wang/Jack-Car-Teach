{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.2em;\n",
       "line-height:1.4em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: -0.4em;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.5em;\n",
       "line-height:1.4em;\n",
       "padding-left:0em;\n",
       "padding-right:3em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.2em;\n",
    "line-height:1.4em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: -0.4em;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.5em;\n",
    "line-height:1.4em;\n",
    "padding-left:0em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python 2/3 compatibility\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Initialization of Jack-Car-Rental DP solver:\n",
    "\n",
    "- What to maintain in cache ?\n",
    "- How to actualy take action ?\n",
    "- How to deal with environment dynamics ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    def __init__(self, use_precomputed=True, conf=None, precomputed_dict=None):\n",
    "        # problem settings\n",
    "        if use_precomputed:\n",
    "            if precomputed_dict is None: \n",
    "                raise ValueError('if \\'use_precomputed\\' is set to True, you have to specify \\'precomputed_dict\\'')\n",
    "            conf_path = osp.abspath(osp.expanduser(precomputed_dict['conf_path']))\n",
    "            conf = np.load(conf_path).tolist() # may not be a good way to load data\n",
    "            self._max_move = conf['max_move']\n",
    "            self._max_cars_A = conf['max_cars_A']\n",
    "            self._max_cars_B = conf['max_cars_B']\n",
    "            self._rent_price = conf['rent_price']\n",
    "            self._move_cost = conf['move_cost']\n",
    "            self._gamma = conf['gamma']\n",
    "            P_path = osp.abspath(osp.expanduser(precomputed_dict['P_path']))\n",
    "            R_path = osp.abspath(osp.expanduser(precomputed_dict['R_path']))\n",
    "        else:\n",
    "            self._max_move = conf['max_move']\n",
    "            self._max_cars_A = conf['max_cars_A']\n",
    "            self._max_cars_B = conf['max_cars_B']\n",
    "            self._rent_price = conf['rent_price']\n",
    "            self._move_cost = conf['move_cost']\n",
    "            self._gamma = conf['gamma']\n",
    "\n",
    "        # number of possible actions, -max_move~max_move\n",
    "        self._n_acts = 2*self._max_move + 1\n",
    "\n",
    "        # size of states, which is the number of all combinations of possible number\n",
    "        # of cars at A and B\n",
    "        state_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "\n",
    "        # deterministic policy which is associated with number of cars in A and B,\n",
    "        # i.e. a table ranging from -max_move~max_move, indicating how many cars\n",
    "        # should be moved from A to B (negative moves = moving from B to A).\n",
    "        self._policy = np.random.randint(low=-self._max_move, high=self._max_move, \n",
    "                                         size=((state_size)), dtype=np.int8)\n",
    "\n",
    "        # state value function following current policy\n",
    "        self._V = np.zeros((state_size), dtype=np.float32)\n",
    "\n",
    "        # FULL environment dynamics\n",
    "        PRfull_shape = tuple((self._n_acts, state_size, state_size))\n",
    "        if use_precomputed:\n",
    "            # load precomputed P_full and R_full\n",
    "            self._P_full = np.load(P_path)\n",
    "            self._R_full = np.load(R_path)\n",
    "        else:\n",
    "            # transition matrix associated with triplet (action, current_state, next_state)\n",
    "            self._P_full = np.zeros(PRfull_shape, dtype=np.float32)\n",
    "            # expected return associated with triplet (action, current_state, next_state)\n",
    "            self._R_full = np.zeros(PRfull_shape, dtype=np.float32)\n",
    "            # compute \n",
    "            self._build_env_dynamics()\n",
    "        \n",
    "        # transition matrix and expected return if we follow current policy. This is just\n",
    "        # partial information of full environment dynamics, and is constructed by internal\n",
    "        # function _get_PR_this_policy\n",
    "        self._P_this_policy = np.zeros((state_size,state_size), dtype=np.float32)\n",
    "        self._R_this_policy = np.zeros((state_size,state_size), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- What to maintain in cache ?\n",
    "    - state\n",
    "    - state value function\n",
    "    - policy\n",
    "- How to actualy take action ?\n",
    "    - look up an action table according to given state\n",
    "- How to deal with environment dynamics ?\n",
    "    - considering all possibilities of cars' moves from one location to the other $\\to$ to compute transition probability\n",
    "    - take rental earns and move costs into consideration $\\to$ to compute reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Policy Iteration:\n",
    "\n",
    "Implementation (with a slight difference) of policy iteration following pseudo-code,\n",
    "    \n",
    "<img src=\"./index/policy_iteration_vis.png\"> <img src=\"./index/policy_iteration.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    def _policy_iteration(self, tol):\n",
    "        '''\n",
    "        FUNC: take one step using policy iteration\n",
    "        '''\n",
    "        # obtain P_this_policy and R_this_policy\n",
    "        self._get_PR_this_policy()\n",
    "\n",
    "        state_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "\n",
    "        ### policy evaluation ###\n",
    "        err = 1e5\n",
    "        V = self._V\n",
    "        while(err>tol):\n",
    "            # compute new value function\n",
    "            V_rep = np.tile(V, (state_size, 1))\n",
    "            new_V = np.sum(self._P_this_policy * (self._R_this_policy + self._gamma*V_rep), axis=1)\n",
    "            # compute SSE (sum of square error)\n",
    "            err = np.sum(np.square(new_V - V))\n",
    "            # update V\n",
    "            V = new_V\n",
    "\n",
    "        ### policy improvement ###\n",
    "        # compute value function of trying different actions in current timestep\n",
    "        score = np.zeros((self._n_acts, state_size))\n",
    "        V_rep = np.tile(V, (state_size, 1))\n",
    "        for act_idx in range(self._n_acts):\n",
    "            score[act_idx] = np.sum(self._P_full[act_idx] * (self._R_full[act_idx] + self._gamma*V_rep), axis=1)\n",
    "        # obtain new policy\n",
    "        new_policy = np.argmax(score, axis=0)\n",
    "        new_policy -= self._max_move # from 0~n_acts to -max_move~max_move\n",
    "\n",
    "        return new_V, new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Value Iteration:\n",
    "\n",
    "Realization of value iteration following pseudo code,\n",
    "\n",
    "<img src=\"./index/value_iteration.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    def _value_iteration(self):\n",
    "        '''\n",
    "        FUNC: take one step using value iteration\n",
    "        '''\n",
    "        state_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "        V = self._V.copy()\n",
    "        policy = np.zeros_like(self._policy)\n",
    "        score = np.zeros((self._n_acts))\n",
    "        for s in range(state_size):\n",
    "            for act_idx in range(self._n_acts):\n",
    "                score[act_idx] = np.sum(self._P_full[act_idx,s] * (self._R_full[act_idx,s] + self._gamma*V))\n",
    "            policy[s] = np.argmax(score) - self._max_move\n",
    "            V[s] = np.max(score)\n",
    "\n",
    "        return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Environment dynamics:\n",
    "\n",
    "Given ($s_t$, $s_{t+1}$, $a_t$), compute transition probability and expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    def _compute_pr(self, n_A, n_B, n_A_next, n_B_next, act):\n",
    "        '''\n",
    "        FUNC: compute transition probability and expected return given triplets (this_state,next_state,action)\n",
    "        '''\n",
    "        # from 0~n_acts to -max_move~max_move, +: A-->B, -: B-->A\n",
    "        act = act - self._max_move\n",
    "        # cars moved from one location to the other cannot > the number of cars at the location\n",
    "        if (act>0 and act>n_A) or (act<0 and -act>n_B):\n",
    "            return 0, 0\n",
    "        # maximum number of cars which can be rented\n",
    "        A_max_rent = n_A - act\n",
    "        B_max_rent = n_B + act\n",
    "        # max_rent cannot surpass maximum storage\n",
    "        if A_max_rent>self._max_cars_A or B_max_rent>self._max_cars_B:\n",
    "            return 0, 0\n",
    "        # difference of cars number in current state(today) and next state(tommorrow)\n",
    "        A_diff = n_A_next - A_max_rent\n",
    "        B_diff = n_B_next - B_max_rent\n",
    "        ### go through all possibilties from cars number as 'n_X' to 'n_X_next' with 'act' done overnight \n",
    "        # in location A\n",
    "        r_A = p_A = 0\n",
    "        for A_rent in range(A_max_rent,-1,-1): # loop from max_rent to 0 --> work with if-break\n",
    "            A_return = A_rent + A_diff\n",
    "            # number of cars returned cannot be negative\n",
    "            if A_return<0:\n",
    "                break\n",
    "            tmp = env.A_return_prob(A_return) * env.A_rent_prob(A_rent)\n",
    "            r_A += (A_rent*self._rent_price) * tmp\n",
    "            p_A += tmp\n",
    "        # in location B\n",
    "        r_B = p_B = 0\n",
    "        for B_rent in range(B_max_rent,-1,-1):\n",
    "            B_return = B_rent + B_diff\n",
    "            # number of cars returned cannot be negative\n",
    "            if B_return<0:\n",
    "                break\n",
    "            tmp = env.B_return_prob(B_return) * env.B_rent_prob(B_rent)\n",
    "            r_B += (B_rent*self._rent_price) * tmp\n",
    "            p_B += tmp\n",
    "        # compute total expected reward and transition probability\n",
    "        r = r_A + r_B - np.absolute(act)*self._move_cost\n",
    "        p = p_A * p_B\n",
    "\n",
    "        return p, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Full class of Jack-Car-Rental DP solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class JackCar(object):\n",
    "    def __init__(self, use_precomputed=True, conf=None, precomputed_dict=None):\n",
    "        '''\n",
    "        FUNC: constructor of JackCar object, if use_precomputed=False, you need to specify conf,\n",
    "              otherwise, you need to specify precomputed_dict\n",
    "        Argument:\n",
    "            use_precomputed: a bool to determine whether precomputed environment dynamics is used\n",
    "            conf: a dictionary specifying configuration of Jack-Car-Rental problem with keys:\n",
    "                  'max_move': maximum number of car moves overnight\n",
    "                  'max_cars_A': maximum number of cars allowed to be stored at A\n",
    "                  'max_cars_B': maximum number of cars allowed to be stored at B\n",
    "                  'rent_price': revenue earned from renting a car\n",
    "                  'move_cost': cost to move a car from A to B or from B to A\n",
    "                  'gamma': discout factor of the MDP\n",
    "            precomputed_dict: a dictionary the same as conf but with extra 2 keys:\n",
    "                  'P_path': path to .npy file containing transition maxtrix of full environment dynamics\n",
    "                  'R_path': path to .npy file containing expected return matrix of full environemnt dynamics\n",
    "        '''\n",
    "        # problem settings\n",
    "        if use_precomputed:\n",
    "            if precomputed_dict is None: \n",
    "                raise ValueError('if \\'use_precomputed\\' is set to True, you have to specify \\'precomputed_dict\\'')\n",
    "            conf_path = osp.abspath(osp.expanduser(precomputed_dict['conf_path']))\n",
    "            conf = np.load(conf_path).tolist() # may not be a good way to load data\n",
    "            self._max_move = conf['max_move']\n",
    "            self._max_cars_A = conf['max_cars_A']\n",
    "            self._max_cars_B = conf['max_cars_B']\n",
    "            self._rent_price = conf['rent_price']\n",
    "            self._move_cost = conf['move_cost']\n",
    "            self._gamma = conf['gamma']\n",
    "            P_path = osp.abspath(osp.expanduser(precomputed_dict['P_path']))\n",
    "            R_path = osp.abspath(osp.expanduser(precomputed_dict['R_path']))\n",
    "        else:\n",
    "            self._max_move = conf['max_move']\n",
    "            self._max_cars_A = conf['max_cars_A']\n",
    "            self._max_cars_B = conf['max_cars_B']\n",
    "            self._rent_price = conf['rent_price']\n",
    "            self._move_cost = conf['move_cost']\n",
    "            self._gamma = conf['gamma']\n",
    "\n",
    "        # number of possible actions, -max_move~max_move\n",
    "        self._n_acts = 2*self._max_move + 1\n",
    "\n",
    "        # size of states, which is the number of all combinations of possible number\n",
    "        # of cars at A and B\n",
    "        state_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "\n",
    "        # deterministic policy which is associated with number of cars in A and B,\n",
    "        # i.e. a table ranging from -max_move~max_move, indicating how many cars\n",
    "        # should be moved from A to B (negative moves = moving from B to A).\n",
    "        self._policy = np.random.randint(low=-self._max_move, high=self._max_move, \n",
    "                                         size=((state_size)), dtype=np.int8)\n",
    "\n",
    "        # state value function following current policy\n",
    "        self._V = np.zeros((state_size), dtype=np.float32)\n",
    "\n",
    "        # FULL environment dynamics\n",
    "        PRfull_shape = tuple((self._n_acts, state_size, state_size))\n",
    "        if use_precomputed:\n",
    "            # load precomputed P_full and R_full\n",
    "            self._P_full = np.load(P_path)\n",
    "            self._R_full = np.load(R_path)\n",
    "        else:\n",
    "            # transition matrix associated with triplet (action, current_state, next_state)\n",
    "            self._P_full = np.zeros(PRfull_shape, dtype=np.float32)\n",
    "            # expected return associated with triplet (action, current_state, next_state)\n",
    "            self._R_full = np.zeros(PRfull_shape, dtype=np.float32)\n",
    "            # compute \n",
    "            self._build_env_dynamics()\n",
    "        \n",
    "        # transition matrix and expected return if we follow current policy. This is just\n",
    "        # partial information of full environment dynamics, and is constructed by internal\n",
    "        # function _get_PR_this_policy\n",
    "        self._P_this_policy = np.zeros((state_size,state_size), dtype=np.float32)\n",
    "        self._R_this_policy = np.zeros((state_size,state_size), dtype=np.float32)\n",
    "\n",
    "    def run(self, method='policy-iteration', tol=1e-8):\n",
    "        '''\n",
    "        FUNC: run DP to obtain optimal policy and corresponding value function\n",
    "        Argument:\n",
    "            method: which DP to solve the planning problem, 'policy-iteration' or 'value-iteration'\n",
    "            tol: tolerance of policy evaluation, only used in method 'policy iteration'\n",
    "        '''\n",
    "        print('Start running DP using method {}'.format(method))\n",
    "        err = 1e5 # just a random large number for initial error\n",
    "        n_iters = 0\n",
    "        while(err!=0):\n",
    "            # run one step\n",
    "            if method=='policy-iteration':\n",
    "                new_V, new_policy = self._policy_iteration(tol)\n",
    "            elif method=='value-iteration':\n",
    "                new_V, new_policy = self._value_iteration()\n",
    "            else:\n",
    "                raise NameError('method in JackCar.run(*) should be \\'policy-iteration\\' or \\'value-iteration\\'')\n",
    "            # check new policy is always better than or equal to the old policy\n",
    "            tmp = new_V - self._V\n",
    "            if tmp[tmp<0].any() and n_iters!=0: # if n_iters==0, we still have no valid V to be compared with\n",
    "                print('Value funtion degenerates, meaning that there must be something wrong in DP')\n",
    "                break\n",
    "            # compute error\n",
    "            err = np.sum(np.absolute(new_policy-self._policy))\n",
    "            print('policy difference = {}'.format(err))\n",
    "            # update policy and value function\n",
    "            self._policy = new_policy\n",
    "            self._V = new_V\n",
    "            n_iters += 1\n",
    "\n",
    "        print('End!! Current policy should be the optimal one')\n",
    "\n",
    "    def visualize_policy(self):\n",
    "        '''\n",
    "        FUNC: visualize policy\n",
    "        '''\n",
    "        policy = self.policy\n",
    "        \n",
    "        print('')\n",
    "        print('Optimal policy:')\n",
    "        for i in range(self._max_cars_A+2):\n",
    "            if i==0:\n",
    "                line = ' 0 '\n",
    "            elif i==int((self._max_cars_A+2)/2):\n",
    "                line = ' A '\n",
    "            else:\n",
    "                line = '   '\n",
    "\n",
    "            if i==self._max_cars_A+1:\n",
    "                line += 'V'\n",
    "            elif i!=0:\n",
    "                line += '|'\n",
    "\n",
    "            for j in range(self._max_cars_B):\n",
    "                if i==0:\n",
    "                    if j==int((self._max_cars_B)/2):\n",
    "                        line += ' B '\n",
    "                    else:\n",
    "                        line += '   '\n",
    "                elif i==1:\n",
    "                    if j==self._max_cars_B-1:\n",
    "                        line += '-->{:3}'.format(self._max_cars_B)\n",
    "                    else:\n",
    "                        line += '---'\n",
    "                else:\n",
    "                    line += '{:3}'.format(policy[i-2][j])\n",
    "            print(line)\n",
    "        print(' {:3}'.format(self._max_cars_A))\n",
    "        \n",
    "        print('+: move cars from A to B')\n",
    "        print('-: move cars from B to A')\n",
    "\n",
    "        print('')\n",
    "\n",
    "    def save_full_env_dynamics(self, fname):\n",
    "        '''\n",
    "        FUNC: save full environment dynamics and problem setting to 3 files,\n",
    "              fname_conf.npy, fname_P.npy, and fname_R.npy\n",
    "        Argument:\n",
    "            fname: prefix filename to be saved as\n",
    "        '''\n",
    "        # save configuration file\n",
    "        conf = {\n",
    "            'max_move': self._max_move,\n",
    "            'max_cars_A': self._max_cars_A,\n",
    "            'max_cars_B': self._max_cars_B,\n",
    "            'rent_price': self._rent_price,\n",
    "            'move_cost': self._move_cost,\n",
    "            'gamma': self._gamma\n",
    "        }\n",
    "        conf_fname = fname + '_conf.npy'\n",
    "        np.save(conf_fname, conf)\n",
    "        # save P_full\n",
    "        P_fname = fname + '_P.npy'\n",
    "        np.save(P_fname, self._P_full)\n",
    "        # save R_full\n",
    "        R_fname = fname + '_R.npy'\n",
    "        np.save(R_fname, self._R_full)\n",
    "\n",
    "        print('Files saved to {}, {}, and {}'.format(conf_fname, P_fname, R_fname))\n",
    "\n",
    "    def _policy_iteration(self, tol):\n",
    "        '''\n",
    "        FUNC: take one step using policy iteration\n",
    "        '''\n",
    "        # obtain P_this_policy and R_this_policy\n",
    "        self._get_PR_this_policy()\n",
    "\n",
    "        state_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "\n",
    "        ### policy evaluation ###\n",
    "        err = 1e5\n",
    "        V = self._V\n",
    "        while(err>tol):\n",
    "            # compute new value function\n",
    "            V_rep = np.tile(V, (state_size, 1))\n",
    "            new_V = np.sum(self._P_this_policy * (self._R_this_policy + self._gamma*V_rep), axis=1)\n",
    "            # compute SSE (sum of square error)\n",
    "            err = np.sum(np.square(new_V - V))\n",
    "            # update V\n",
    "            V = new_V\n",
    "\n",
    "        ### policy improvement ###\n",
    "        # compute value function of trying different actions in current timestep\n",
    "        score = np.zeros((self._n_acts, state_size))\n",
    "        V_rep = np.tile(V, (state_size, 1))\n",
    "        for act_idx in range(self._n_acts):\n",
    "            score[act_idx] = np.sum(self._P_full[act_idx] * (self._R_full[act_idx] + self._gamma*V_rep), axis=1)\n",
    "        # obtain new policy\n",
    "        new_policy = np.argmax(score, axis=0)\n",
    "        new_policy -= self._max_move # from 0~n_acts to -max_move~max_move\n",
    "\n",
    "        return new_V, new_policy\n",
    "\n",
    "    def _value_iteration(self):\n",
    "        '''\n",
    "        FUNC: take one step using value iteration\n",
    "        '''\n",
    "        state_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "        V = self._V.copy()\n",
    "        policy = np.zeros_like(self._policy)\n",
    "        score = np.zeros((self._n_acts))\n",
    "        for s in range(state_size):\n",
    "            for act_idx in range(self._n_acts):\n",
    "                score[act_idx] = np.sum(self._P_full[act_idx,s] * (self._R_full[act_idx,s] + self._gamma*V))\n",
    "            policy[s] = np.argmax(score) - self._max_move\n",
    "            V[s] = np.max(score)\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "    def _get_PR_this_policy(self):\n",
    "        '''\n",
    "        FUNC: get transition matrix and expected return according to current policy\n",
    "        '''\n",
    "        # from -max_move~max_move to 0~n_acts\n",
    "        act_idx = self._policy + self._max_move\n",
    "        # obtain P and R following current policy\n",
    "        AB_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "        for i in range(AB_size):\n",
    "            for j in range(AB_size):\n",
    "                self._P_this_policy[i,j] = self._P_full[act_idx[i], i, j]\n",
    "                self._R_this_policy[i,j] = self._R_full[act_idx[i], i, j]\n",
    "\n",
    "    def _build_env_dynamics(self):\n",
    "        '''\n",
    "        FUNC: build full environment dynamics, i.e. transition matrix and return matrix\n",
    "        '''\n",
    "        print('Forming full environment dynamics, P_full and R_full')\n",
    "        tic1 = time.time()\n",
    "        AB_size = (self._max_cars_A+1) * (self._max_cars_B+1)\n",
    "        tmp = self._max_cars_B + 1\n",
    "        for i in range(AB_size):\n",
    "            n_A = int(i / tmp)\n",
    "            n_B = i % tmp\n",
    "            for j in range(AB_size):\n",
    "                n_A_next = int(j / tmp)\n",
    "                n_B_next = j % tmp\n",
    "                for act in range(self._n_acts):\n",
    "                    p, r = self._compute_pr(n_A, n_B, n_A_next, n_B_next, act)\n",
    "                    self._P_full[act,i,j] = p\n",
    "                    self._R_full[act,i,j] = r\n",
    "        toc1 = time.time()\n",
    "        print('End! Elapsed time = {}'.format(toc1-tic1))\n",
    "\n",
    "    def _compute_pr(self, n_A, n_B, n_A_next, n_B_next, act):\n",
    "        '''\n",
    "        FUNC: compute transition probability and expected return given triplets (this_state,next_state,action)\n",
    "        '''\n",
    "        # from 0~n_acts to -max_move~max_move, +: A-->B, -: B-->A\n",
    "        act = act - self._max_move\n",
    "        # cars moved from one location to the other cannot > the number of cars at the location\n",
    "        if (act>0 and act>n_A) or (act<0 and -act>n_B):\n",
    "            return 0, 0\n",
    "        # maximum number of cars which can be rented\n",
    "        A_max_rent = n_A - act\n",
    "        B_max_rent = n_B + act\n",
    "        # max_rent cannot surpass maximum storage\n",
    "        if A_max_rent>self._max_cars_A or B_max_rent>self._max_cars_B:\n",
    "            return 0, 0\n",
    "        # difference of cars number in current state(today) and next state(tommorrow)\n",
    "        A_diff = n_A_next - A_max_rent\n",
    "        B_diff = n_B_next - B_max_rent\n",
    "        ### go through all possibilties from cars number as 'n_X' to 'n_X_next' with 'act' done overnight \n",
    "        # in location A\n",
    "        r_A = p_A = 0\n",
    "        for A_rent in range(A_max_rent,-1,-1): # loop from max_rent to 0 --> work with if-break\n",
    "            A_return = A_rent + A_diff\n",
    "            # number of cars returned cannot be negative\n",
    "            if A_return<0:\n",
    "                break\n",
    "            tmp = env.A_return_prob(A_return) * env.A_rent_prob(A_rent)\n",
    "            r_A += (A_rent*self._rent_price) * tmp\n",
    "            p_A += tmp\n",
    "        # in location B\n",
    "        r_B = p_B = 0\n",
    "        for B_rent in range(B_max_rent,-1,-1):\n",
    "            B_return = B_rent + B_diff\n",
    "            # number of cars returned cannot be negative\n",
    "            if B_return<0:\n",
    "                break\n",
    "            tmp = env.B_return_prob(B_return) * env.B_rent_prob(B_rent)\n",
    "            r_B += (B_rent*self._rent_price) * tmp\n",
    "            p_B += tmp\n",
    "        # compute total expected reward and transition probability\n",
    "        r = r_A + r_B - np.absolute(act)*self._move_cost\n",
    "        p = p_A * p_B\n",
    "\n",
    "        return p, r\n",
    "\n",
    "    # property decorator\n",
    "    @property\n",
    "    def policy(self):\n",
    "        n_A = self._max_cars_A + 1\n",
    "        n_B = self._max_cars_B + 1\n",
    "        return np.reshape(self._policy, (n_A,n_B))\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self._V\n",
    "    @property\n",
    "    def max_move(self):\n",
    "        return self._max_move\n",
    "    @property\n",
    "    def max_cars_A(self):\n",
    "        return self._max_cars_A\n",
    "    @property\n",
    "    def max_cars_B(self):\n",
    "        return self._max_cars_B\n",
    "    @property\n",
    "    def rent_price(self):\n",
    "        return self._rent_price\n",
    "    @property\n",
    "    def move_cost(self):\n",
    "        return self._move_cost\n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
